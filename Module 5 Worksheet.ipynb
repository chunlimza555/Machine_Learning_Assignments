{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuTq8S-cN1rm"
   },
   "source": [
    "##  Module 5 Worksheet - Chapter 9\n",
    "\n",
    "The three checkpoints included in this worksheet need to be completed and marked during your lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cu5L-yROg3r"
   },
   "source": [
    "### Checkpoint 1 - Cross Validation\n",
    "\n",
    "Load the California Housing regression dataset (<code>datasets.fetch_california_housing()</code>) and train a KNeighborsRegressor, LinearRegression and DecisionTreeRegressor models to predict the median house price of a block group instance.\n",
    "\n",
    "Compare the results and runtimes when performing the following model evaluation procedures:\n",
    "- Evaluate using train/test splitting (holdout 10% for testing)\n",
    "- Evaluate using K-fold cross validation (try for K = 10, 100 and 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "KHiJvuldPRZz",
    "outputId": "adeb6b58-4bb0-43a4-94ef-80328d29f670",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train/Test Split Evaluation ---\n",
      "  LinearRegression     | R-squared: 0.5808 | Runtime: 0.0313s\n",
      "  KNeighborsRegressor  | R-squared: 0.1712 | Runtime: 0.0545s\n",
      "  DecisionTreeRegressor | R-squared: 0.6357 | Runtime: 0.3290s\n"
     ]
    }
   ],
   "source": [
    "# Enter your code for Checkpoint 1 here\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Load and Prepare the Data ---\n",
    "housing = datasets.fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data into training and testing sets (10% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize the three models\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"KNeighborsRegressor\": KNeighborsRegressor(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "# Evaluate using Train/Test Splitting \n",
    "print(\"--- Train/Test Split Evaluation ---\")\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Calculate the R-squared score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    runtime = time.time() - start_time\n",
    "    print(f\"  {name:<20} | R-squared: {r2:.4f} | Runtime: {runtime:.4f}s\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- K-Fold Cross-Validation Evaluation ---\n",
      "\n",
      "Evaluating with K = 10...\n",
      "  LinearRegression     | Mean R-squared: 0.6001 | Std R-squared: 0.0222 | Runtime: 3.5031s\n",
      "  KNeighborsRegressor  | Mean R-squared: 0.1684 | Std R-squared: 0.0190 | Runtime: 1.9828s\n",
      "  DecisionTreeRegressor | Mean R-squared: 0.6039 | Std R-squared: 0.0167 | Runtime: 0.5911s\n",
      "\n",
      "Evaluating with K = 100...\n",
      "  LinearRegression     | Mean R-squared: 0.6013 | Std R-squared: 0.0631 | Runtime: 0.3571s\n",
      "  KNeighborsRegressor  | Mean R-squared: 0.1732 | Std R-squared: 0.0688 | Runtime: 0.6696s\n",
      "  DecisionTreeRegressor | Mean R-squared: 0.6073 | Std R-squared: 0.0789 | Runtime: 4.0962s\n",
      "\n",
      "Evaluating with K = 1000...\n",
      "  LinearRegression     | Mean R-squared: 0.5646 | Std R-squared: 0.2585 | Runtime: 1.3147s\n",
      "  KNeighborsRegressor  | Mean R-squared: 0.1011 | Std R-squared: 0.2748 | Runtime: 4.5254s\n",
      "  DecisionTreeRegressor | Mean R-squared: 0.5558 | Std R-squared: 0.2970 | Runtime: 38.8834s\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using K-Fold Cross-Validation\n",
    "print(\"\\n--- K-Fold Cross-Validation Evaluation ---\")\n",
    "\n",
    "# Define the values for K to test\n",
    "k_values = [10, 100, 1000]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nEvaluating with K = {k}...\")\n",
    "    \n",
    "    # Create a new KFold object for each 'k'\n",
    "    cv = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        start_time = time.time()\n",
    "        # Perform cross-validation with 'r2' scoring and shuffle=True\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X,\n",
    "            y,\n",
    "            cv=cv,  \n",
    "            scoring=\"r2\",\n",
    "            n_jobs=-1,  # Use all available CPU cores for speed\n",
    "        )\n",
    "        runtime = time.time() - start_time\n",
    "        \n",
    "        r2_mean = np.mean(scores)\n",
    "        r2_std = np.std(scores)\n",
    "        print(\n",
    "            f\"  {name:<20} | Mean R-squared: {r2_mean:.4f} | Std R-squared: {r2_std:.4f} | Runtime: {runtime:.4f}s\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBIBFBKFQRaz"
   },
   "source": [
    "### Checkpoint 2 - Model Evaluation Metrics\n",
    "\n",
    "Load the UCI Breast Cancer Wisconsin (Diagnostic) classification dataset (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) and train a RandomForestClassifier model to predict whether the cancer is malignant (0) or benign (1).\n",
    "\n",
    "Evaluate the performance of the model using the following metrics (use stratified 10-fold cross validation):\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score\n",
    "\n",
    "---\n",
    "\n",
    "We can create a naive (dummy) classifier model that always predicts the most common label using the following code:\n",
    "\n",
    "```python\n",
    "from sklearn.dummy import DummyClassifier\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "dc = DummyClassifier(strategy = 'most_frequent')\n",
    "dc.fit(breast_cancer.data, breast_cancer.target)\n",
    "```\n",
    "\n",
    "For this dataset, the DummyClassifier model will always output 1 regardless of what the input feature vector is.\n",
    "\n",
    "Calculate the Accuracy, Precision, Recall and F1-Score for this DummyClassifier model (use stratified 10-fold cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "QESzzG8-HDbV",
    "outputId": "34cf3cb7-66c9-4731-ca2c-24f4611ab80a"
   },
   "outputs": [],
   "source": [
    "# Enter your code for Checkpoint 2 here\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Load the dataset\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# Create a Dummy Classifier\n",
    "# A DummyClassifier is a baseline model used to compare against \"real\" models.\n",
    "# Here we use strategy='most_frequent', which always predicts the majority class\n",
    "# found in the training set. This provides a minimum benchmark for accuracy.\n",
    "dc = DummyClassifier(strategy='most_frequent')\n",
    "dc.fit(breast_cancer.data, breast_cancer.target)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare Features (X) and Labels (y)\n",
    "# X contains the input features (e.g., cell nucleus characteristics).\n",
    "# y contains the target labels (0 = malignant, 1 = benign).\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the classifiers\n",
    "# A powerful ensemble method that builds multiple decision trees and\n",
    "# combines their predictions for better accuracy and robustness.\n",
    "# Setting random_state=42 ensures reproducible results.\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Dummy Classifier\n",
    "# A baseline model that makes simple predictions without using input features.\n",
    "# By default, strategy='prior' (predicts classes according to training distribution).\n",
    "# Setting random_state=42 makes its behavior reproducible if random strategies are used.\n",
    "dc_classifier = DummyClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Set up stratified 10-fold cross-validation\n",
    "# StratifiedKFold ensures that each fold has approximately the same percentage\n",
    "# of samples from each class as the original dataset.\n",
    "# This is important for imbalanced datasets like breast cancer (more benign than malignant).\n",
    "# - n_splits=10 → split the dataset into 10 folds\n",
    "# - shuffle=True → shuffle data before splitting (prevents bias from ordering)\n",
    "# - random_state=42 → ensures the same shuffling every run (reproducibility)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RandomForestClassifier Evaluation ---\n",
      "  Accuracy  : Mean = 0.9561, Std = 0.0239\n",
      "  Precision : Mean = 0.9627, Std = 0.0331\n",
      "  Recall    : Mean = 0.9692, Std = 0.0232\n",
      "  F1        : Mean = 0.9654, Std = 0.0183\n"
     ]
    }
   ],
   "source": [
    "# Define the scoring metrics\n",
    "# Evaluating the model using four common classification metrics:\n",
    "# - accuracy:   Overall proportion of correct predictions\n",
    "# - precision:  Of the positive predictions, how many were correct\n",
    "# - recall:     Of the actual positives, how many were detected\n",
    "# - f1:         Harmonic mean of precision and recall (balances both)\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the RandomForestClassifier\n",
    "print(\"--- RandomForestClassifier Evaluation ---\")\n",
    "\n",
    "# Loop through each scoring metric\n",
    "for metric in scoring:\n",
    "    # Perform stratified 10-fold cross-validation\n",
    "    # - rf_classifier: the model being evaluated\n",
    "    # - X, y: features and target labels\n",
    "    # - cv=skf: use stratified 10-fold CV (keeps class balance in each fold)\n",
    "    # - scoring=metric: specify which performance metric to calculate\n",
    "    # - n_jobs=-1: use all CPU cores for faster computation\n",
    "    scores = cross_val_score(rf_classifier, X, y, cv=skf, scoring=metric, n_jobs=-1)\n",
    "    \n",
    "    # Print mean and standard deviation of the metric across the 10 folds\n",
    "    # - Mean shows average model performance\n",
    "    # - Std shows variability (how stable the model is across folds)\n",
    "    print(f\"  {metric.capitalize():<10}: Mean = {np.mean(scores):.4f}, Std = {np.std(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DummyClassifier Evaluation ---\n",
      "  Accuracy  : Mean = 0.6274, Std = 0.0070\n",
      "  Precision : Mean = 0.6274, Std = 0.0070\n",
      "  Recall    : Mean = 1.0000, Std = 0.0000\n",
      "  F1        : Mean = 0.7710, Std = 0.0053\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the DummyClassifier\n",
    "print(\"\\n--- DummyClassifier Evaluation ---\")\n",
    "\n",
    "# Iterate through each metric in the 'scoring' list\n",
    "# The 'scoring' list likely contains strings like 'accuracy', 'f1', 'precision', etc.\n",
    "for metric in scoring:\n",
    "    # Perform cross-validation to get performance scores for the DummyClassifier\n",
    "    # 'dc_classifier': The DummyClassifier model instance\n",
    "    # 'X': The feature data\n",
    "    # 'y': The target labels\n",
    "    # 'cv=skf': Use StratifiedKFold cross-validation for balanced folds\n",
    "    # 'scoring=metric': Use the current metric from the loop for evaluation\n",
    "    # 'n_jobs=-1': Use all available CPU cores for faster processing\n",
    "    scores = cross_val_score(dc_classifier, X, y, cv=skf, scoring=metric, n_jobs=-1)\n",
    "    \n",
    "    # Print the mean and standard deviation of the scores for the current metric\n",
    "    # The mean shows the average performance, and the standard deviation shows the variability\n",
    "    # {metric.capitalize():<10}: Format the metric name with a capital letter and left-align it\n",
    "    # {np.mean(scores):.4f}: Format the mean score to 4 decimal places\n",
    "    # {np.std(scores):.4f}: Format the standard deviation to 4 decimal places\n",
    "    print(f\"  {metric.capitalize():<10}: Mean = {np.mean(scores):.4f}, Std = {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEb7I91HHems"
   },
   "source": [
    "### Checkpoint 3 - Grid Search\n",
    "\n",
    "Load the California Housing regression dataset (<code>datasets.fetch_california_housing()</code>) and train a RandomForestRegressor model that predicts the median house price of a block group instance. Use the following Grid Search approach to identify the most effective combination of hyperparameters:\n",
    "\n",
    "1) Call \"np.random.seed(42)\" to fix the random seed.\n",
    "2) Split the data into training and test sets (test_size = 0.2).\n",
    "3) Standardise the feature values for the training and test sets, based on the training set values.\n",
    "4) Define a RandomForestRegressor object, and a KFold object (use K = 5 and shuffle the data order).\n",
    "5) Define the following Dictionary object, that specifies the possible hyperparameter values that will be evaluated: <code>grids = {'n_estimators': [10, 50, 100], 'min_samples_leaf': [2, 10]}</code>.\n",
    "6) Define a GridSearchCV object, using the previously defined RandomForestRegressor, KFold and Dictionary (grids) objects as arguments, with the scoring metric to 'r2'. Hint, set the parameter <code>n_jobs=-1</code> to parallelize the evaluations and reduce program runtime.\n",
    "7) Fit this GridSearchCV object to the training features and labels.\n",
    "8) Report the hyperparameter combination that produces the highest R^2 score.\n",
    "\n",
    "---\n",
    "\n",
    "Repeat the above process on the Iris dataset for a distance weighted K-neighbours classifier with the following hyperparameters space:\n",
    "- distance metric = [euclidean, cosine, manhattan, minkowski]\n",
    "- K = [1, 3, 5, 10, 50]\n",
    "\n",
    "Use accuracy as the scoring metric for comparing hyperparameter combinations.\n",
    "Try using both 'accuracy' and 'f1_macro' (f1 score with handling for multi-class targets) as the scoring metrics for comparing hyperparameter combinations. Do you get a different \"best\" hyperparameter combination for each scoring metric?\n",
    "\n",
    "---\n",
    "\n",
    "Repeat the above process on the UCI Breast Cancer Wisconsin (Diagnostic) dataset for a decision tree classifier with the following hyperparameters space:\n",
    "- max_depth = [1, 2, ..., 49, 50]\n",
    "- min_samples_split = [2, 3, ..., 31, 32]\n",
    "- min_samples_leaf = [1, 2, ..., 49, 50]\n",
    "- max_leaf_nodes = [2, 3, ..., 127, 128]\n",
    "\n",
    "Note, unless your computer is VERY powerful, the sheer number of possible combinations for the above hyperparameter values makes regular cross-validation Grid Search infeasible to perform. Instead, you should use RandomizedSearchCV to randomly sample from the space of possible hyperparameter combinations. You can select the number of hyperparameter combinations that are sampled by changing the \"n_iter\" parameter (recommend starting out with n_iter=1000).\n",
    "\n",
    "Try using both 'accuracy' and 'f1' (f1 score) as the scoring metrics for comparing hyperparameter combinations. Do you get a different \"best\" hyperparameter combination for each scoring metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code for Checkpoint 3 here\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "california_housing = datasets.fetch_california_housing()\n",
    "X, y = california_housing.data, california_housing.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splite to test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['malignant' 'benign']\n",
      "Counts for each class: [212 357]\n",
      "------------------------------\n",
      "The most frequent class is 'benign' with 357 instances.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# 1. Load the dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "y = breast_cancer.target  # The target variable contains the class labels\n",
    "\n",
    "# 2. Count the occurrences of each class\n",
    "unique_classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# 3. Find the most frequent class\n",
    "most_frequent_class_index = np.argmax(counts)\n",
    "most_frequent_class_label = unique_classes[most_frequent_class_index]\n",
    "most_frequent_class_name = breast_cancer.target_names[most_frequent_class_label]\n",
    "\n",
    "# 4. Print the results\n",
    "print(f\"Class labels: {breast_cancer.target_names}\")\n",
    "print(f\"Counts for each class: {counts}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"The most frequent class is '{most_frequent_class_name}' with {counts[most_frequent_class_index]} instances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RandomForestRegressor\n",
    "# RandomForestRegressor is an ensemble model that builds multiple decision trees\n",
    "# and averages their predictions to improve accuracy and reduce overfitting.\n",
    "# Setting random_state=42 ensures reproducibility (same results each run).\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Define the KFold cross-validation strategy\n",
    "# KFold will split the dataset into 5 folds (subsets).\n",
    "# - n_splits=5: divides data into 5 equal parts (80% train, 20% test per fold)\n",
    "# - shuffle=True: randomly shuffle the data before splitting to reduce bias\n",
    "# - random_state=42: ensures consistent shuffling every time for reproducibility\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "# The grid specifies different combinations of hyperparameters to try\n",
    "# during model selection (GridSearchCV).\n",
    "# - n_estimators: number of trees in the forest (10, 50, 100)\n",
    "# - min_samples_leaf: minimum number of samples required in a leaf node (2, 10)\n",
    "# Grid search will test all possible combinations of these values.\n",
    "grids = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'min_samples_leaf': [2, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing: Best hyperparameters for R2 score: {'min_samples_leaf': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Initialize GridSearchCV\n",
    "# GridSearchCV will perform an exhaustive search over the hyperparameter grid.\n",
    "# Parameters:\n",
    "# - rfr: the RandomForestRegressor model we defined earlier\n",
    "# - grids: the dictionary of hyperparameter values to try\n",
    "# - cv=kfold: use 5-fold cross-validation for evaluation\n",
    "# - scoring='r2': optimize based on the R² score (coefficient of determination)\n",
    "# - n_jobs=-1: use all available CPU cores for faster computation\n",
    "gscv = GridSearchCV(rfr, grids, cv=kfold, scoring='r2', n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the GridSearchCV to training data\n",
    "# Fit the model to the scaled training dataset.\n",
    "# - X_train_scaled: training features (standardized/scaled)\n",
    "# - y_train: training target values\n",
    "# During this step:\n",
    "#   - GridSearchCV will try every combination of hyperparameters in `grids`\n",
    "#   - For each combination, it will run cross-validation (5 folds)\n",
    "#   - It evaluates each model using the R² score\n",
    "#   - The best-performing set of hyperparameters is stored\n",
    "gscv.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "# After fitting, GridSearchCV stores the best parameter set in `.best_params_`\n",
    "# Here we print the combination of n_estimators and min_samples_leaf\n",
    "# that gave the highest R² score during cross-validation.\n",
    "print(\"California Housing: Best hyperparameters for R2 score:\", gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Nearest Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------------------\n",
    "# Create a pipeline with scaling + KNN\n",
    "# ------------------------------\n",
    "\n",
    "# A Pipeline allows us to chain multiple preprocessing and modeling steps\n",
    "# so they are executed together inside cross-validation (avoiding data leakage).\n",
    "# Steps in this pipeline:\n",
    "# - 'scaler': StandardScaler() standardizes the features \n",
    "#             (mean=0, variance=1) → important for distance-based models like KNN\n",
    "# - 'knn': KNeighborsClassifier() performs classification based on nearest neighbors\n",
    "knn_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),       # Step 1: Standardize input features\n",
    "    ('knn', KNeighborsClassifier())     # Step 2: Apply KNN classifier\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Define hyperparameter grid for KNN\n",
    "# Using the double underscore (__) notation to access parameters\n",
    "# of the 'knn' step inside the pipeline.\n",
    "# - 'knn__metric': Distance metrics to test:\n",
    "#     * 'euclidean'  → straight-line distance\n",
    "#     * 'cosine'     → similarity based on angle between vectors\n",
    "#     * 'manhattan'  → sum of absolute differences (a.k.a. L1 norm)\n",
    "#     * 'minkowski'  → generalization of Euclidean and Manhattan (controlled by p)\n",
    "# - 'knn__n_neighbors': Number of nearest neighbors to consider for classification.\n",
    "#   Trying a range from very local (1) to broader (50).\n",
    "grids_knn = {\n",
    "    'knn__metric': ['euclidean', 'cosine', 'manhattan', 'minkowski'],\n",
    "    'knn__n_neighbors': [1, 3, 5, 10, 50]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iris Dataset: Best hyperparameters for Accuracy: {'knn__metric': 'manhattan', 'knn__n_neighbors': 10}\n"
     ]
    }
   ],
   "source": [
    "# Set up GridSearchCV for Accuracy\n",
    "# GridSearchCV will try every combination of hyperparameters from grids_knn\n",
    "# and evaluate them using 5-fold cross-validation.\n",
    "# Parameters:\n",
    "# - knn_pipe: pipeline (scaler + KNN) ensures preprocessing happens within CV\n",
    "# - grids_knn: hyperparameter grid (metrics + number of neighbors)\n",
    "# - cv=5: 5-fold cross-validation (train on 80%, validate on 20%, repeat 5 times)\n",
    "# - scoring='accuracy': optimize for classification accuracy\n",
    "# - n_jobs=-1: use all available CPU cores to run in parallel for speed\n",
    "gscv_acc = GridSearchCV(knn_pipe, grids_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the GridSearchCV to the training data\n",
    "# Train and evaluate KNN models using all parameter combinations\n",
    "# - X_train_iris: training features from the Iris dataset\n",
    "# - y_train_iris: training labels (species of Iris flowers)\n",
    "# During this step, GridSearchCV:\n",
    "#   - Runs 5-fold cross-validation for each parameter combination\n",
    "#   - Computes average accuracy for each\n",
    "#   - Stores the best parameter combination\n",
    "gscv_acc.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "\n",
    "\n",
    "# Print the best hyperparameters\n",
    "# The attribute .best_params_ contains the hyperparameter set that\n",
    "# achieved the highest mean accuracy score across CV folds.\n",
    "print(\"\\nIris Dataset: Best hyperparameters for Accuracy:\", gscv_acc.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset: Best hyperparameters for F1-macro: {'knn__metric': 'manhattan', 'knn__n_neighbors': 10}\n"
     ]
    }
   ],
   "source": [
    "# Set up GridSearchCV for F1-macro\n",
    "# GridSearchCV will test every combination of hyperparameters in grids_knn\n",
    "# and evaluate them using 5-fold cross-validation.\n",
    "# Parameters:\n",
    "# - knn_pipe: pipeline (scaling + KNN model)\n",
    "# - grids_knn: hyperparameter grid (distance metrics + number of neighbors)\n",
    "# - cv=5: 5-fold cross-validation (split data into 5 train/validation folds)\n",
    "# - scoring='f1_macro': optimize for macro-averaged F1 score\n",
    "#       * Macro F1 = average of F1 scores across all classes (equal weight)\n",
    "#       * Good for multi-class classification like Iris (3 species)\n",
    "# - n_jobs=-1: use all CPU cores to speed up computation\n",
    "gscv_f1 = GridSearchCV(knn_pipe, grids_knn, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "# Fit the GridSearchCV to training data\n",
    "# Train and evaluate KNN models using all parameter combinations\n",
    "# - X_train_iris: training features from the Iris dataset\n",
    "# - y_train_iris: training labels (species of Iris flowers)\n",
    "# GridSearchCV will:\n",
    "#   - Run 5-fold CV for each parameter combination\n",
    "#   - Compute the average F1-macro score\n",
    "#   - Select the parameter set with the highest score\n",
    "gscv_f1.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "\n",
    "\n",
    "# Print the best hyperparameters\n",
    "# .best_params_ stores the hyperparameter set that produced the\n",
    "# highest mean F1-macro score across cross-validation folds.\n",
    "print(\"Iris Dataset: Best hyperparameters for F1-macro:\", gscv_f1.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "# Load Breast Cancer Dataset\n",
    "# The breast cancer dataset is built into scikit-learn.\n",
    "# - Features: computed from cell nuclei of breast cancer biopsies\n",
    "# - Target: 0 = malignant, 1 = benign\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X_bc, y_bc = breast_cancer.data, breast_cancer.target\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Decision Tree Classifier\n",
    "# DecisionTreeClassifier is a simple, interpretable model.\n",
    "# Setting random_state=42 ensures reproducibility (consistent splits).\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Define Hyperparameter Search Space\n",
    "# Instead of testing every possible value (GridSearch),\n",
    "# we use RandomizedSearchCV with random distributions.\n",
    "# Each parameter is sampled from a specified range (using scipy.stats.randint).\n",
    "grids_dtc = {\n",
    "    'max_depth': randint(1, 51),          # Depth of the tree (1–50)\n",
    "    'min_samples_split': randint(2, 33),  # Minimum samples required to split a node (2–32)\n",
    "    'min_samples_leaf': randint(1, 51),   # Minimum samples in a leaf node (1–50)\n",
    "    'max_leaf_nodes': randint(2, 129)     # Maximum number of leaf nodes (2–128)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Breast Cancer: Best hyperparameters for Accuracy: {'max_depth': 38, 'max_leaf_nodes': 110, 'min_samples_leaf': 8, 'min_samples_split': 28}\n"
     ]
    }
   ],
   "source": [
    "# Randomized Search for Accuracy\n",
    "# RandomizedSearchCV will sample random combinations of hyperparameters\n",
    "# from the specified distributions (grids_dtc) instead of exhaustively testing all.\n",
    "# Parameters:\n",
    "# - dtc: the DecisionTreeClassifier model\n",
    "# - grids_dtc: dictionary of hyperparameter distributions (randint ranges)\n",
    "# - n_iter=1000: number of random combinations to try (larger = more thorough, but slower)\n",
    "# - cv=5: 5-fold cross-validation (ensures balanced evaluation)\n",
    "# - scoring='accuracy': optimize hyperparameters based on accuracy\n",
    "# - n_jobs=-1: use all CPU cores for faster computation\n",
    "# - random_state=42: ensures reproducibility (same random samples across runs)\n",
    "rscv_acc = RandomizedSearchCV(\n",
    "    dtc, grids_dtc, n_iter=1000, cv=5,\n",
    "    scoring='accuracy', n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fit the RandomizedSearchCV to the dataset\n",
    "# Fit the model on the entire breast cancer dataset.\n",
    "# RandomizedSearchCV will:\n",
    "#   - Randomly sample 1000 hyperparameter combinations\n",
    "#   - For each combination, run 5-fold cross-validation\n",
    "#   - Compute accuracy for each fold\n",
    "#   - Select the hyperparameters with the best mean accuracy\n",
    "rscv_acc.fit(X_bc, y_bc)\n",
    "\n",
    "\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "# The attribute .best_params_ stores the set of hyperparameters\n",
    "# that achieved the highest average accuracy across cross-validation folds.\n",
    "print(\"\\nBreast Cancer: Best hyperparameters for Accuracy:\", rscv_acc.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer: Best hyperparameters for F1 score: {'max_depth': 38, 'max_leaf_nodes': 110, 'min_samples_leaf': 8, 'min_samples_split': 28}\n"
     ]
    }
   ],
   "source": [
    "# Randomized Search for F1 score\n",
    "# RandomizedSearchCV will optimize hyperparameters for the DecisionTreeClassifier,\n",
    "# this time using the F1 score as the evaluation metric.\n",
    "# Parameters:\n",
    "# - dtc: DecisionTreeClassifier model\n",
    "# - grids_dtc: dictionary of hyperparameter distributions (randint ranges)\n",
    "# - n_iter=1000: number of random hyperparameter combinations to sample\n",
    "# - cv=5: 5-fold cross-validation\n",
    "# - scoring='f1': use binary F1 score (harmonic mean of precision & recall)\n",
    "#                 F1 is especially useful when the dataset is imbalanced\n",
    "# - n_jobs=-1: use all CPU cores for faster computation\n",
    "# - random_state=42: reproducible results\n",
    "rscv_f1 = RandomizedSearchCV(\n",
    "    dtc, grids_dtc, n_iter=1000, cv=5,\n",
    "    scoring='f1', n_jobs=-1, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the RandomizedSearchCV to the dataset\n",
    "# Fit the model on the full breast cancer dataset.\n",
    "# RandomizedSearchCV will:\n",
    "#   - Randomly sample 1000 hyperparameter sets\n",
    "#   - Train + validate using 5-fold CV\n",
    "#   - Compute F1 score for each combination\n",
    "#   - Select the hyperparameters with the highest mean F1 score\n",
    "rscv_f1.fit(X_bc, y_bc)\n",
    "\n",
    "\n",
    "\n",
    "# Print the best hyperparameters\n",
    "# The attribute .best_params_ gives the hyperparameters that achieved\n",
    "# the best mean F1 score across all folds.\n",
    "print(\"Breast Cancer: Best hyperparameters for F1 score:\", rscv_f1.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
